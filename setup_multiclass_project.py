#!/usr/bin/env python3
# setup_multiclass_project.py - Setup multi-class classification project

import os
import shutil

def create_multiclass_project_structure():
    """Create project structure for multi-class classification"""
    print("üöÄ SETTING UP MULTI-CLASS CLASSIFICATION PROJECT")
    print("=" * 60)
    
    # Define project structure
    project_name = "multiclass_eeg_classification"
    
    folders = [
        f"{project_name}",
        f"{project_name}/data",
        f"{project_name}/src",
        f"{project_name}/models", 
        f"{project_name}/results",
        f"{project_name}/figures",
        f"{project_name}/tables",
        f"{project_name}/notebooks",
        f"{project_name}/docs"
    ]
    
    # Create folders
    for folder in folders:
        os.makedirs(folder, exist_ok=True)
        print(f"  ‚úÖ Created: {folder}")
    
    return project_name

def copy_relevant_files(project_name):
    """Copy relevant files from binary classification project"""
    print(f"\nüìÅ COPYING RELEVANT FILES TO {project_name}")
    print("=" * 60)
    
    # Files to copy and modify
    files_to_copy = [
        "convert_data.py",
        "advanced_wavelet_features.py",
        "hybrid_cnn_lstm_attention.py",
        "ensemble_model.py"
    ]
    
    # Copy files
    for file in files_to_copy:
        if os.path.exists(file):
            dest_path = f"{project_name}/src/{file}"
            shutil.copy2(file, dest_path)
            print(f"  ‚úÖ Copied: {file} -> {dest_path}")
    
    # Copy data folder
    if os.path.exists("Data"):
        dest_data = f"{project_name}/data/Data"
        if not os.path.exists(dest_data):
            shutil.copytree("Data", dest_data)
            print(f"  ‚úÖ Copied: Data -> {dest_data}")

def create_multiclass_data_loader(project_name):
    """Create multi-class data loader"""
    content = '''#!/usr/bin/env python3
# multiclass_data_loader.py - Load and prepare multi-class EEG data (0-9)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt

def load_multiclass_data(data_path="data/Data/EP1.01.txt", max_samples_per_digit=500):
    """
    Load multi-class EEG data for digits 0-9
    
    Args:
        data_path: Path to MindBigData file
        max_samples_per_digit: Maximum samples per digit class
    
    Returns:
        X: EEG data array
        y: Labels (0-9)
        metadata: Additional information
    """
    print("üîÑ Loading Multi-class EEG Data (Digits 0-9)")
    print("=" * 50)
    
    all_data = []
    all_labels = []
    metadata = {'channels': [], 'sample_counts': {}}
    
    # Initialize counters for each digit
    digit_counts = {i: 0 for i in range(10)}
    
    with open(data_path, 'r') as f:
        for line_num, line in enumerate(f):
            if line_num % 10000 == 0:
                print(f"  Processing line {line_num}...")
            
            if not line.strip():
                continue
                
            try:
                parts = line.split('\\t')
                if len(parts) >= 7:
                    digit = int(parts[4])
                    channel = parts[3]
                    data_string = parts[6]
                    
                    # Only process digits 0-9
                    if digit in range(10):
                        # Check if we need more samples for this digit
                        if digit_counts[digit] < max_samples_per_digit:
                            # Parse EEG data
                            values = [float(x.strip()) for x in data_string.split(',') if x.strip()]
                            
                            if len(values) > 100:  # Minimum length requirement
                                all_data.append(values)
                                all_labels.append(digit)
                                digit_counts[digit] += 1
                                
                                if channel not in metadata['channels']:
                                    metadata['channels'].append(channel)
                    
                    # Stop if we have enough samples for all digits
                    if all(count >= max_samples_per_digit for count in digit_counts.values()):
                        break
                        
            except (ValueError, IndexError) as e:
                continue
    
    # Convert to numpy arrays
    X = np.array(all_data)
    y = np.array(all_labels)
    
    # Store sample counts
    metadata['sample_counts'] = digit_counts
    
    print(f"\\nüìä Data Loading Summary:")
    print(f"  Total samples: {len(X)}")
    print(f"  Features per sample: {X.shape[1] if len(X) > 0 else 0}")
    print(f"  Classes: {len(np.unique(y))}")
    
    for digit in range(10):
        count = np.sum(y == digit)
        print(f"  Digit {digit}: {count} samples")
    
    return X, y, metadata

def preprocess_multiclass_data(X, y, target_length=128):
    """
    Preprocess multi-class EEG data
    
    Args:
        X: Raw EEG data
        y: Labels
        target_length: Target sequence length
    
    Returns:
        X_processed: Processed EEG data
        y_processed: Processed labels
    """
    print(f"\\nüîß Preprocessing Multi-class Data")
    print("=" * 50)
    
    X_processed = []
    y_processed = []
    
    for i, (sample, label) in enumerate(zip(X, y)):
        # Normalize length
        if len(sample) >= target_length:
            # Truncate
            processed_sample = sample[:target_length]
        else:
            # Pad with zeros
            processed_sample = np.pad(sample, (0, target_length - len(sample)), 'constant')
        
        # Normalize amplitude
        processed_sample = (processed_sample - np.mean(processed_sample)) / (np.std(processed_sample) + 1e-8)
        
        X_processed.append(processed_sample)
        y_processed.append(label)
    
    X_processed = np.array(X_processed)
    y_processed = np.array(y_processed)
    
    print(f"  Processed shape: {X_processed.shape}")
    print(f"  Label distribution: {np.bincount(y_processed)}")
    
    return X_processed, y_processed

def create_multiclass_splits(X, y, test_size=0.2, val_size=0.1, random_state=42):
    """Create train/validation/test splits for multi-class data"""
    print(f"\\nüìä Creating Multi-class Data Splits")
    print("=" * 50)
    
    # First split: train+val vs test
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    # Second split: train vs val
    val_size_adjusted = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, stratify=y_temp
    )
    
    print(f"  Train set: {X_train.shape[0]} samples")
    print(f"  Validation set: {X_val.shape[0]} samples") 
    print(f"  Test set: {X_test.shape[0]} samples")
    
    # Check class distribution
    for split_name, y_split in [("Train", y_train), ("Val", y_val), ("Test", y_test)]:
        print(f"  {split_name} distribution: {np.bincount(y_split)}")
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def main():
    """Main function to demonstrate multi-class data loading"""
    # Load data
    X, y, metadata = load_multiclass_data()
    
    if len(X) > 0:
        # Preprocess
        X_processed, y_processed = preprocess_multiclass_data(X, y)
        
        # Create splits
        X_train, X_val, X_test, y_train, y_val, y_test = create_multiclass_splits(X_processed, y_processed)
        
        # Save processed data
        np.save("multiclass_data.npy", X_processed)
        np.save("multiclass_labels.npy", y_processed)
        
        print(f"\\n‚úÖ Multi-class data preparation completed!")
        print(f"üìÅ Saved: multiclass_data.npy, multiclass_labels.npy")
    else:
        print("‚ùå No data loaded. Please check the data path.")

if __name__ == "__main__":
    main()
'''
    
    file_path = f"{project_name}/src/multiclass_data_loader.py"
    with open(file_path, 'w') as f:
        f.write(content)
    
    print(f"  ‚úÖ Created: {file_path}")

def create_multiclass_models(project_name):
    """Create multi-class specific models"""
    
    # Multi-class CNN-LSTM model
    cnn_lstm_content = '''#!/usr/bin/env python3
# multiclass_cnn_lstm.py - Multi-class CNN-LSTM model for digits 0-9

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class MultiClassCNNLSTM(nn.Module):
    """Multi-class CNN-LSTM model for 10-digit classification"""
    
    def __init__(self, input_length=128, num_classes=10, dropout_rate=0.5):
        super(MultiClassCNNLSTM, self).__init__()
        
        # CNN layers for feature extraction
        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, padding=3)
        self.bn1 = nn.BatchNorm1d(64)
        self.pool1 = nn.MaxPool1d(2)
        
        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm1d(128)
        self.pool2 = nn.MaxPool1d(2)
        
        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm1d(256)
        self.pool3 = nn.MaxPool1d(2)
        
        # Calculate LSTM input size
        lstm_input_size = input_length // 8  # After 3 pooling layers
        
        # LSTM layers
        self.lstm = nn.LSTM(256, 128, num_layers=2, batch_first=True, 
                           dropout=dropout_rate, bidirectional=True)
        
        # Attention mechanism
        self.attention = nn.MultiheadAttention(256, num_heads=8, dropout=dropout_rate)
        
        # Classification layers
        self.dropout = nn.Dropout(dropout_rate)
        self.fc1 = nn.Linear(256, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, num_classes)
        
    def forward(self, x):
        # CNN feature extraction
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        
        x = F.relu(self.bn3(self.conv3(x)))
        x = self.pool3(x)
        
        # Reshape for LSTM
        batch_size = x.size(0)
        x = x.permute(0, 2, 1)  # (batch, seq_len, features)
        
        # LSTM
        lstm_out, (h_n, c_n) = self.lstm(x)
        
        # Attention
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # Global average pooling
        x = torch.mean(attn_out, dim=1)
        
        # Classification
        x = self.dropout(x)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        
        return x

def train_multiclass_model(model, train_loader, val_loader, num_epochs=100, device='cpu'):
    """Train multi-class model with advanced techniques"""
    
    # Loss function with class weights for imbalanced data
    criterion = nn.CrossEntropyLoss()
    
    # Optimizer with learning rate scheduling
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10
    )
    
    # Early stopping
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 15
    
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            train_total += target.size(0)
            train_correct += (predicted == target).sum().item()
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()
        
        # Calculate metrics
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        
        # Learning rate scheduling
        scheduler.step(val_loss)
        
        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # Save best model
            torch.save(model.state_dict(), 'best_multiclass_model.pth')
        else:
            patience_counter += 1
        
        if epoch % 10 == 0:
            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        
        if patience_counter >= patience:
            print(f'Early stopping at epoch {epoch}')
            break
    
    return train_losses, val_losses, train_accs, val_accs

def evaluate_multiclass_model(model, test_loader, device='cpu'):
    """Evaluate multi-class model"""
    model.eval()
    all_predictions = []
    all_targets = []
    all_probabilities = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            probabilities = F.softmax(output, dim=1)
            _, predicted = torch.max(output, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
    
    # Calculate metrics
    accuracy = np.mean(np.array(all_predictions) == np.array(all_targets))
    
    # Classification report
    report = classification_report(all_targets, all_predictions, 
                                 target_names=[f'Digit {i}' for i in range(10)])
    
    # Confusion matrix
    cm = confusion_matrix(all_targets, all_predictions)
    
    return accuracy, report, cm, all_probabilities

def main():
    """Main function for multi-class training"""
    print("üöÄ Multi-class CNN-LSTM Training")
    print("=" * 50)
    
    # This would be called with actual data loaders
    print("Model architecture created for 10-digit classification")
    print("Expected accuracy: 25-40% (based on literature)")
    print("Key challenges: Class imbalance, inter-class similarity, noise")

if __name__ == "__main__":
    main()
'''
    
    file_path = f"{project_name}/src/multiclass_cnn_lstm.py"
    with open(file_path, 'w') as f:
        f.write(cnn_lstm_content)
    
    print(f"  ‚úÖ Created: {file_path}")

def create_project_readme(project_name):
    """Create README for multi-class project"""
    
    readme_content = f'''# Multi-Class EEG Digit Classification (0-9)

## Overview
This project extends the binary EEG classification (6 vs 9) to a full multi-class classification task for digits 0-9. This represents a significantly more challenging problem with expected accuracy in the 25-40% range based on literature.

## Project Structure
```
{project_name}/
‚îú‚îÄ‚îÄ data/                   # Data files
‚îú‚îÄ‚îÄ src/                    # Source code
‚îú‚îÄ‚îÄ models/                 # Trained models
‚îú‚îÄ‚îÄ results/                # Results and metrics
‚îú‚îÄ‚îÄ figures/                # Publication figures
‚îú‚îÄ‚îÄ tables/                 # Publication tables
‚îú‚îÄ‚îÄ notebooks/              # Jupyter notebooks
‚îî‚îÄ‚îÄ docs/                   # Documentation
```

## Key Differences from Binary Classification

### Complexity Increase
- **Classes**: 10 (digits 0-9) vs 2 (digits 6, 9)
- **Decision Boundaries**: 45 (one-vs-one) vs 1
- **Random Baseline**: 10% vs 50%
- **Expected Accuracy**: 25-40% vs 80%+

### Technical Challenges
1. **Class Imbalance**: Uneven distribution across digits
2. **Inter-class Similarity**: Many digits have similar EEG patterns
3. **Increased Noise**: Signal-to-noise ratio becomes critical
4. **Overfitting**: More prone to memorization
5. **Confidence Distribution**: Much lower confidence scores

## Methodology

### Enhanced Data Processing
- Balanced sampling across all 10 digits
- Advanced data augmentation techniques
- Robust preprocessing pipeline

### Advanced Model Architecture
- Deeper CNN-LSTM-Attention networks
- Multi-head attention mechanisms
- Regularization techniques (dropout, batch norm)
- Class-weighted loss functions

### Ensemble Approaches
- Hierarchical classification (binary trees)
- One-vs-rest ensemble
- Confidence-based voting
- Meta-learning with uncertainty quantification

## Expected Results

### Literature Benchmarks
- Kaongoen & Jo (2017): 31.2%
- Bird et al. (2019): 28.7%
- Spampinato et al. (2017): 40.0%

### Target Performance
- **Primary Goal**: 35-45% accuracy
- **Stretch Goal**: 45-50% accuracy
- **Confidence**: Realistic distribution (0.3-0.7)

## Research Questions

1. How does classification difficulty scale with number of classes?
2. Which digits are most/least distinguishable in EEG?
3. Can hierarchical approaches improve performance?
4. What is the role of attention in multi-class EEG classification?
5. How does confidence distribution change with task complexity?

## Publication Strategy

### Target Journals
- IEEE Transactions on Neural Systems and Rehabilitation Engineering
- Journal of Neural Engineering
- Frontiers in Human Neuroscience
- IEEE Access

### Key Contributions
1. Comprehensive multi-class EEG classification framework
2. Hierarchical ensemble approaches
3. Attention mechanism analysis for EEG
4. Confidence calibration for BCI applications
5. Scalability analysis of EEG classification

## Usage

```bash
# Load and preprocess multi-class data
python src/multiclass_data_loader.py

# Train multi-class models
python src/multiclass_cnn_lstm.py

# Run ensemble experiments
python src/multiclass_ensemble.py

# Generate results and figures
python src/generate_results.py
```

## Future Work
- Real-time multi-class BCI implementation
- Cross-subject validation
- Integration with other modalities
- Clinical applications

## References
1. Kaongoen, N., & Jo, S. (2017). A novel online BCI system using CNN
2. Bird, J. J., et al. (2019). Mental emotional sentiment classification
3. Spampinato, C., et al. (2017). Deep learning human mind for automated visual classification

---
*This project builds upon the successful binary classification work and represents the next step toward practical multi-class BCI applications.*
'''
    
    file_path = f"{project_name}/README.md"
    with open(file_path, 'w') as f:
        f.write(readme_content)
    
    print(f"  ‚úÖ Created: {file_path}")

def main():
    """Main function to setup multi-class project"""
    # Create project structure
    project_name = create_multiclass_project_structure()
    
    # Copy relevant files
    copy_relevant_files(project_name)
    
    # Create new files
    print(f"\nüîß CREATING NEW FILES FOR MULTI-CLASS")
    print("=" * 60)
    
    create_multiclass_data_loader(project_name)
    create_multiclass_models(project_name)
    create_project_readme(project_name)
    
    print(f"\n‚úÖ MULTI-CLASS PROJECT SETUP COMPLETED!")
    print("=" * 60)
    print(f"üìÅ Project created: {project_name}/")
    print(f"üéØ Next steps:")
    print(f"  1. cd {project_name}")
    print(f"  2. python src/multiclass_data_loader.py")
    print(f"  3. python src/multiclass_cnn_lstm.py")
    print(f"  4. Develop advanced ensemble methods")
    print(f"  5. Generate publication materials")
    
    print(f"\nüìä Expected Outcomes:")
    print(f"  - Accuracy: 25-40% (vs 10% random)")
    print(f"  - Novel multi-class ensemble framework")
    print(f"  - Publication in top-tier journal")
    print(f"  - Foundation for real-world BCI applications")

if __name__ == "__main__":
    main()
